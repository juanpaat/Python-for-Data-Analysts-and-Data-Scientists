{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data tools and Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Accuracy:** How close you are to the actual value you're looking for.   \n",
    "- **Precision:** How reproducible and repeatible is your algorithm.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Data?  \n",
    "\n",
    "Any type of information, any piece of information.  \n",
    "\n",
    "\n",
    "\n",
    "![Img](https://monkeylearn.com/static/306f5989991713a9d77314a514e161c2/899e8/image5.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an Algorithm?  \n",
    "\n",
    "Series of steps you take in order to accomplish a task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Data Science?  \n",
    "In order to understan what Data Science is, first we need to undertand what Science is. **Science** in the knowledge or a system of knowledge especially as obtained and tested through **scientific method**. **Data Science** then is the sciences that uses data to drive conclusions using the scientific method.  \n",
    "It is very improtant to have in mind that part of the scientific method consist in testing Hypothesis but also in refusing Hipothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?\n",
    "Machine learning is a tool of Artificial intelligence that gives the machines the ability of lean wihtout being explicitly  programmed. Before **ML** (Machine learning), we had to explicitly define all possible actiones that the machine should take given a specific situation.  \n",
    "ML is often used when traditional statistic can not deal with a task, or when the amough of variables and/or data is massive.   \n",
    "\n",
    "\n",
    "There are many uses for ML, such as:\n",
    "\n",
    "* Prediction, like forcasting demand or prices changes.\n",
    "* Image recognition, like face detection.\n",
    "* Medical diagnoses, such as the ability to recognize cancerous tissues or anomalies in medical images. \n",
    "* Finance, performing fraud investigations and credit checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Artificial Intelligence?\n",
    "Artificial intelligence, or AI for short, has been a field of study since the 1950’s. Popularity has increased recently because of the advancements in computing power.\n",
    "\n",
    "But what is artificial intelligence? AI is the ability to enable computers to understand data, learn from the data, and make decisions based on patterns hidden in the data. This sounds a lot like Machine Learning. In fact, machine learning tools are implementations of AI. These ML tools perform tasks that could otherwise be very difficult, if not impossible, for humans to perform manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# important Tools for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Sciences is a reconised and popular field nowadays, however it wasn't always like that. It is now in part due to the amoun of data that is collected and available to be analized today.  Today, we produce about 2.5 quintillion (2,500,000,000,000,000,000) bytes of data daily. We are now are able to analyze data in a scale that some years ago would have sounded like science fiction, thanks to the advances in Technology.\n",
    "\n",
    "<center><img src=\"Images/ds_venn_diagram.jpeg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Every project begins with a question!\n",
    "\n",
    "What will this stock be worth tomorrow? What brand of cars appeals more to 18-year-olds? Why is my website traffic plummeting? A question or set of questions allow us to frame each part of a project.\n",
    "\n",
    "After we have decided we want to answer some question, we will need to build a **data pipeline** that allows us to get answers. A full data pipeline usually requires some combination of the following:\n",
    "\n",
    "\n",
    "![](https://i.ytimg.com/vi/cugl5t-W1sE/maxresdefault.jpg)\n",
    "\n",
    "\n",
    "\n",
    "- **Procurement**: In order to use data, we must first find it. This may be as simple as buying it from an existing source, or as complex as creating a whole economic endeavor that mines data from user behavior. Some business problems can be solved with a singular data source, but many other need data from many different sources.  This uses a lot of coding and domain knowledge.\n",
    "\n",
    "- **Aggregation**: Once we have acquired our data, we must aggregate it. This usually entails creating a process by which we create more focused sets of data from our original information. One example you may be familiar with is getting the grades of a student and his attendance record together in one single spreadsheet. Sometimes this entails very complex processes such as natural language processing, and some others it may be as simple as joining 2 datasets from a simple id. For this we need mainly coding.\n",
    "\n",
    "- **Cleaning**: We must also clean all this data. Sensors are not perfect, human-inputted values are many times erroneous, flukes of luck and chance affect some outcomes. Whatever the reason, in the real world it is extremely rare to find a dataset without some manner of outliers, data which is suspect. Sometimes before or sometimes after aggregation, we must take care to clean our data of such oddities in order to make sure our conclusions are sound. This uses coding, statistics and domain knowledge in equal measures.\n",
    "\n",
    "- **Analysis**: After we have orderly and clean data, we can analyze it. This means using a complete assortment of tools in order to extract meaning and actionable insights from our information. While computers help, this is usually the one part of the process on which humans are still unbeatable. This is done with domain knowledge for the most part, with a bit of statistics and coding sprinkled on top.\n",
    "\n",
    "- **Modeling**: Using our insights and domain knowledge, we can now proceed to create models which allows us to predict future states of the world given the information we've gathered about past states of the world. Models may be as simple as Linear Regressions, or as complex as Ensemble Neural networks. In the end, you may think as all models as a box in which you input an incomplete set of information about the state of a bit of the world -be that the real estate market or the sales projections for the next year- and which outputs a prediction about the present or future of that little piece of existence. All 3 fields of knowledge are mandatory here.\n",
    "\n",
    "- **Presentation**: Finally, we must explain our process, insights and findings. This is often the most underrated skill in novice data scientists, but in many an elder and wise practitioner's mind is the most important of all. After all, our insights and models will not be very actionable if we can't explain them and convince others to act on them. This includes the verbal, written and graphical representation of what we want to convey. Hence, it is advisable that if one is to learn to be great at something in data, that it is the presentation of it.  Domain knowledge is our main asset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The modern data pipeline\n",
    "\n",
    "> \"A data pipeline is a set of actions that ingest raw data from disparate sources and move the data to a destination for storage and analysis. A pipeline also may include filtering and features that provide resiliency against failure.\" \n",
    ">\n",
    "> Source: [What is a data pipeline?](https://www.stitchdata.com/resources/what-is-data-pipeline/)\n",
    "\n",
    "<br>\n",
    "\n",
    "Starting with this simple definition above, we see that the concept of a \"pipeline\" consists of at least three major phases:\n",
    "\n",
    "* the **ingestion** of the data from its source\n",
    "* the **output** or destination of the data (including the storage and analysis)\n",
    "* the **processing** of the data, including multiple cleaning and transformation steps\n",
    "\n",
    "Each of these phases can consist of  multiple steps depending on the format of the data, business requirements, etc. For example, if your team needs to analyze data coming from multiple sources, then there needs to be extra processing steps that transform the data sources into a standardized format that allows them to be joined on common fields. Or, if your data pipeline is designed to serve a large dashboard to be used by many different stakeholders, then your processed data will likely need to be loaded into a database at the end of the pipeline that is [optimized for analytics](https://searchbusinessanalytics.techtarget.com/definition/analytic-database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/oKixNpz6jNo\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7feaa0adac40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame('https://www.youtube.com/embed/oKixNpz6jNo', width=560, height=315 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases\n",
    "\n",
    "<img src=\"Images/logo_dbs.png\">\n",
    "\n",
    "All this information must go somewhere! Once you procured your data, it must be stored somewhere safe. The same goes for the result of all your transformations. For this purpose, we have **databases**.\n",
    "\n",
    "Databases are organized collections of information that represent some amount of data. They are usually in the form of **tables** of information, like a spreadsheet, where each row in the spreadsheet is one instance of some part of our data, and each column is a characteristic for that instance. Each database can have many of these tables. Tables are usually linked to each other as well, in what is known as a **relational database**, and it is by far the most commonplace structure of all.\n",
    "\n",
    "### SQL and PostgreSQL\n",
    "\n",
    "In order to interact with databases, we have the **SQL (Structured Query Language)** programming language. After Python, SQL is the second most important programming language in the field. It allows us to interact directly with a database, by using a series of commands to insert, delete, update, or query information. Almost every relational database can be interacted with through SQL:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM students;\n",
    "WHERE Name='Leela';\n",
    "```\n",
    "\n",
    "There are many variations on the architecture and internal functioning of databases. These variations are known as **Relational Database Management Systems**, or RDBMS for short. You may have heard about some of them, such as MySQL, Oracle, SQL Server and SQLite. Among those, of special note is PostgreSQL. PostgreSQL is free, open-source and extremely powerful, which makes it a very attractive option for new projects. It is the database system used by companies such as Instagram, Twitch, and Skype. It even has a very nice graphical interface which allows you to query and observe your data with little to no coding experience, and is one of the top options to get your feet wet with databases in general. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Languages\n",
    "\n",
    "\n",
    "<center><img src=\"Images/programming_languages.jpeg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why python\n",
    "1. It is easy to learn\n",
    "2. It is easy to read. Many other programming languages are not!\n",
    "3. Python is **open-source** software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDE (integrated development environment)\n",
    " <center><img src=\"Images/ides.png\" width=700 height=300></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "Now that we have procured, transformed, and saved our information in our trusty database, we are ready to model the world with it. **Modeling** is the art of creating predictions for the future or the present out of observations from the past. The way in which we model the world in data science is very much rooted in statistics - everything from the simplest models to the most complex (e.g. neural networks) is just applied statistics at its core.\n",
    "\n",
    "One advantage we have as data scientists is that most models have been generalized and coded for us by previous geniuses in the field. This allows almost anyone to apply complex models such as Gradient-Boosted Trees, Neural Networks, Hierarchical Clustering Algorithms, and many others with only a couple lines of code.\n",
    "\n",
    "#### `scikit-learn`\n",
    "\n",
    "The Swiss Army Knife of modeling is a Python package called `scikit-learn`. `scikit-learn` allows us to run many types of classification, regression, and clustering models with only a couple of lines of Python code.\n",
    "\n",
    "However, in order to use it you must know Python (and `pandas`) as well. Linear Regression? Logistic Regression? Decision Trees? Ensembles? `scikit-learn` has it all. Most of its models are part of the bigger field of **machine learning**, in which you input tons and tons of data into your computer in order to get as accurate a model as possible.\n",
    "\n",
    "A nice thing about `scikit-learn` is its ease of use, which allows even people who have no idea what a model does internally to test it out anyways. Many aspiring data scientists were inspired to learn the math behind the models after seeing the results of them first-hand. This allows even non-math inclined people to dip their toes in machine learning without much time commitment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Libraries\n",
    " <center><img src=\"Images/top-libraries.jpeg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git + Github\n",
    "<center><img src=\"Images/git.png\" width=800 height=400 ></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "During the time we spend together with our information, we must make sure to get to know it deeply. Seldom do we find datasets that offer all their valuable insights freely. By analyzing our information in detail, we can gather insights which allow us to act on the information, to better develop our data pipeline, to improve our models, and to emerge triumphant in our quest of finding answers to our goal question. \n",
    "\n",
    "To aid us in this endeavor, we have tools which allow us to visualize the data. These were some of the first pieces of software built for the field, back when **Business Intelligence (BI)** was all the rage. Excel is probably the most known (but also nowadays the most scorned). After Excel, many other great tools came out that made data analysis and visualization a breeze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Images/logo_tableau_power_bi.jpeg\" width=800 height=200></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the public [Tableau](https://public.tableau.com/en-us/gallery/) and [Power BI](https://community.powerbi.com/t5/Data-Stories-Gallery/bd-p/DataStoriesGallery) galleries for you to see the power of each!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infrastructure\n",
    "\n",
    "Many parts of this work, in particular data transformation and modeling, require a decent amount of computing power. This means that in order to keep your data pipeline flowing, you must acquire machines capable of running all the processes and machinations of your data team.\n",
    "\n",
    "The set of computers needed to run your data pipeline is known as your **infrastructure**. The computers that make up your infrastructure each has its own specifications - its processor, the amount of RAM and disk space it has available, whether or not it has a GPU and what kind, among some other details. These requirements will grow as you build more data-intensive and complex models. The field of work relating to managing the infrastructure for a data pipeline is known as **DataOps**.\n",
    "\n",
    "Now, your infrastructure can be local (commonly referred to as **on-prem**) or in the **cloud**. When your infrastructure is local, it means that you bought or rented computers and have them available geographically close to you (i.e. you may have your own data center full of servers). This means that you must maintain them, keep them cool with AC, pick out the parts for each computer, and keep a team to make sure everything is running smoothly.\n",
    "\n",
    "The alternative is having everything in the cloud. This means nothing more than renting a computer on someone else’s premises and connecting to it over the Internet. Then, you run all your processes on that computer the same as if it was local, but you don't have to worry about maintenance costs and such.\n",
    "\n",
    "Having local infrastructure is usually finicky, time-consuming and has a large upfront cost, which makes it infeasible for major projects. It is for this reason that most professionals nowadays rent their infrastructure and run their projects on the cloud from any of several cloud providers.\n",
    "\n",
    "#### AWS\n",
    "\n",
    "<center><img src=\"Images/logo_aws.png\"></center>\n",
    "\n",
    "Amazon Web Services, or AWS, is the most well-known of the current cloud providers. It offers computing power, storage, and many other specialized services so that you can think about your business instead of hardware.\n",
    "AWS may be the most famous, but its far from the only one. Other options include Google Cloud Platform, Microsoft Azure, among others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "95d86cc39d5d58d521500cebfc408ba0f6686fc300eaf2bd5ed8b0712fdc01ba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
