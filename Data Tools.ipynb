{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data tools and Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Accuracy:** How close you are to the actual value you're looking for.   \n",
    "- **Precision:** How reproducible and repeatible is your algorithm.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Data?  \n",
    "\n",
    "Any type of information, any piece of information.  \n",
    "\n",
    "\n",
    "\n",
    "![Img](https://monkeylearn.com/static/306f5989991713a9d77314a514e161c2/899e8/image5.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an Algorithm?  \n",
    "\n",
    "Series of steps you take in order to accomplish a task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Data Science?  \n",
    "In order to understan what Data Science is, first we need to undertand what Science is. **Science** in the knowledge or a system of knowledge especially as obtained and tested through **scientific method**. **Data Science** then is the sciences that uses data to drive conclusions using the scientific method.  \n",
    "It is very improtant to have in mind that part of the scientific method consist in testing Hypothesis but also in refusing Hipothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Machine Learning?\n",
    "Machine learning is a tool of Artificial intelligence that gives the machines the ability of lean wihtout being explicitly  programmed. Before **ML** (Machine learning), we had to explicitly define all possible actiones that the machine should take given a specific situation.  \n",
    "ML is often used when traditional statistic can not deal with a task, or when the amough of variables and/or data is massive.   \n",
    "\n",
    "\n",
    "There are many uses for ML, such as:\n",
    "\n",
    "* Prediction, like forcasting demand or prices changes.\n",
    "* Image recognition, like face detection.\n",
    "* Medical diagnoses, such as the ability to recognize cancerous tissues or anomalies in medical images. \n",
    "* Finance, performing fraud investigations and credit checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Artificial Intelligence?\n",
    "Artificial intelligence, or AI for short, has been a field of study since the 1950â€™s. Popularity has increased recently because of the advancements in computing power.\n",
    "\n",
    "But what is artificial intelligence? AI is the ability to enable computers to understand data, learn from the data, and make decisions based on patterns hidden in the data. This sounds a lot like Machine Learning. In fact, machine learning tools are implementations of AI. These ML tools perform tasks that could otherwise be very difficult, if not impossible, for humans to perform manually. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# important Tools for "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Sciences is a reconised and popular field nowadays, however it wasn't always like that. It is now in part due to the amoun of data that is collected and available to be analized today.  Today, we produce about 2.5 quintillion (2,500,000,000,000,000,000) bytes of data daily. We are now are able to analyze data in a scale that some years ago would have sounded like science fiction, thanks to the advances in Technology.\n",
    "\n",
    "<center><img src=\"Images/ds_venn_diagram.jpeg\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Every project begins with a question!\n",
    "\n",
    "What will this stock be worth tomorrow? What brand of cars appeals more to 18-year-olds? Why is my website traffic plummeting? A question or set of questions allow us to frame each part of a project.\n",
    "\n",
    "After we have decided we want to answer some question, we will need to build a **data pipeline** that allows us to get answers. A full data pipeline usually requires some combination of the following:\n",
    "\n",
    "\n",
    "![](https://i.ytimg.com/vi/cugl5t-W1sE/maxresdefault.jpg)\n",
    "\n",
    "\n",
    "\n",
    "- **Procurement**: In order to use data, we must first find it. This may be as simple as buying it from an existing source, or as complex as creating a whole economic endeavor that mines data from user behavior. Some business problems can be solved with a singular data source, but many other need data from many different sources.  This uses a lot of coding and domain knowledge.\n",
    "\n",
    "- **Aggregation**: Once we have acquired our data, we must aggregate it. This usually entails creating a process by which we create more focused sets of data from our original information. One example you may be familiar with is getting the grades of a student and his attendance record together in one single spreadsheet. Sometimes this entails very complex processes such as natural language processing, and some others it may be as simple as joining 2 datasets from a simple id. For this we need mainly coding.\n",
    "\n",
    "- **Cleaning**: We must also clean all this data. Sensors are not perfect, human-inputted values are many times erroneous, flukes of luck and chance affect some outcomes. Whatever the reason, in the real world it is extremely rare to find a dataset without some manner of outliers, data which is suspect. Sometimes before or sometimes after aggregation, we must take care to clean our data of such oddities in order to make sure our conclusions are sound. This uses coding, statistics and domain knowledge in equal measures.\n",
    "\n",
    "- **Analysis**: After we have orderly and clean data, we can analyze it. This means using a complete assortment of tools in order to extract meaning and actionable insights from our information. While computers help, this is usually the one part of the process on which humans are still unbeatable. This is done with domain knowledge for the most part, with a bit of statistics and coding sprinkled on top.\n",
    "\n",
    "- **Modeling**: Using our insights and domain knowledge, we can now proceed to create models which allows us to predict future states of the world given the information we've gathered about past states of the world. Models may be as simple as Linear Regressions, or as complex as Ensemble Neural networks. In the end, you may think as all models as a box in which you input an incomplete set of information about the state of a bit of the world -be that the real estate market or the sales projections for the next year- and which outputs a prediction about the present or future of that little piece of existence. All 3 fields of knowledge are mandatory here.\n",
    "\n",
    "- **Presentation**: Finally, we must explain our process, insights and findings. This is often the most underrated skill in novice data scientists, but in many an elder and wise practitioner's mind is the most important of all. After all, our insights and models will not be very actionable if we can't explain them and convince others to act on them. This includes the verbal, written and graphical representation of what we want to convey. Hence, it is advisable that if one is to learn to be great at something in data, that it is the presentation of it.  Domain knowledge is our main asset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The modern data pipeline\n",
    "\n",
    "> \"A data pipeline is a set of actions that ingest raw data from disparate sources and move the data to a destination for storage and analysis. A pipeline also may include filtering and features that provide resiliency against failure.\" \n",
    ">\n",
    "> Source: [What is a data pipeline?](https://www.stitchdata.com/resources/what-is-data-pipeline/)\n",
    "\n",
    "<br>\n",
    "\n",
    "Starting with this simple definition above, we see that the concept of a \"pipeline\" consists of at least three major phases:\n",
    "\n",
    "* the **ingestion** of the data from its source\n",
    "* the **output** or destination of the data (including the storage and analysis)\n",
    "* the **processing** of the data, including multiple cleaning and transformation steps\n",
    "\n",
    "Each of these phases can consist of  multiple steps depending on the format of the data, business requirements, etc. For example, if your team needs to analyze data coming from multiple sources, then there needs to be extra processing steps that transform the data sources into a standardized format that allows them to be joined on common fields. Or, if your data pipeline is designed to serve a large dashboard to be used by many different stakeholders, then your processed data will likely need to be loaded into a database at the end of the pipeline that is [optimized for analytics](https://searchbusinessanalytics.techtarget.com/definition/analytic-database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"560\"\n",
       "            height=\"315\"\n",
       "            src=\"https://www.youtube.com/embed/oKixNpz6jNo\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7feaa0adac40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame('https://www.youtube.com/embed/oKixNpz6jNo', width=560, height=315 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Databases\n",
    "\n",
    "<img src=\"Images/logo_dbs.png\">\n",
    "\n",
    "All this information must go somewhere! Once you procured your data, it must be stored somewhere safe. The same goes for the result of all your transformations. For this purpose, we have **databases**.\n",
    "\n",
    "Databases are organized collections of information that represent some amount of data. They are usually in the form of **tables** of information, like a spreadsheet, where each row in the spreadsheet is one instance of some part of our data, and each column is a characteristic for that instance. Each database can have many of these tables. Tables are usually linked to each other as well, in what is known as a **relational database**, and it is by far the most commonplace structure of all.\n",
    "\n",
    "### SQL and PostgreSQL\n",
    "\n",
    "In order to interact with databases, we have the **SQL (Structured Query Language)** programming language. After Python, SQL is the second most important programming language in the field. It allows us to interact directly with a database, by using a series of commands to insert, delete, update, or query information. Almost every relational database can be interacted with through SQL:\n",
    "\n",
    "```sql\n",
    "SELECT * FROM students;\n",
    "WHERE Name='Leela';\n",
    "```\n",
    "\n",
    "There are many variations on the architecture and internal functioning of databases. These variations are known as **Relational Database Management Systems**, or RDBMS for short. You may have heard about some of them, such as MySQL, Oracle, SQL Server and SQLite. Among those, of special note is PostgreSQL. PostgreSQL is free, open-source and extremely powerful, which makes it a very attractive option for new projects. It is the database system used by companies such as Instagram, Twitch, and Skype. It even has a very nice graphical interface which allows you to query and observe your data with little to no coding experience, and is one of the top options to get your feet wet with databases in general. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Languages\n",
    "\n",
    "\n",
    "<img src=\"Images/programming_languages.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why python\n",
    "1. It is easy to learn\n",
    "2. It is easy to read. Many other programming languages are not!\n",
    "3. Python is **open-source** software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDE (integrated development environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "Now that we have procured, transformed, and saved our information in our trusty database, we are ready to model the world with it. **Modeling** is the art of creating predictions for the future or the present out of observations from the past. The way in which we model the world in data science is very much rooted in statistics - everything from the simplest models to the most complex (e.g. neural networks) is just applied statistics at its core.\n",
    "\n",
    "One advantage we have as data scientists is that most models have been generalized and coded for us by previous geniuses in the field. This allows almost anyone to apply complex models such as Gradient-Boosted Trees, Neural Networks, Hierarchical Clustering Algorithms, and many others with only a couple lines of code.\n",
    "\n",
    "#### `scikit-learn`\n",
    "\n",
    "The Swiss Army Knife of modeling is a Python package called `scikit-learn`. `scikit-learn` allows us to run many types of classification, regression, and clustering models with only a couple of lines of Python code.\n",
    "\n",
    "However, in order to use it you must know Python (and `pandas`) as well. Linear Regression? Logistic Regression? Decision Trees? Ensembles? `scikit-learn` has it all. Most of its models are part of the bigger field of **machine learning**, in which you input tons and tons of data into your computer in order to get as accurate a model as possible.\n",
    "\n",
    "A nice thing about `scikit-learn` is its ease of use, which allows even people who have no idea what a model does internally to test it out anyways. Many aspiring data scientists were inspired to learn the math behind the models after seeing the results of them first-hand. This allows even non-math inclined people to dip their toes in machine learning without much time commitment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Libraries\n",
    " <img src=\"Images/top-libraries.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Git + Github\n",
    "#<img src=\"Images/git-branches-merge.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
